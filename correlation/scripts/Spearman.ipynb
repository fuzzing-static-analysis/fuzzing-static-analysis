{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af07807e-2d27-4b26-a4f7-b64bfc1ac02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "TIME = 82800\n",
    "\n",
    "# Important note!\n",
    "# If you want to compare libafl-based fuzzer then exclude aflpp, honggfuzz, libfuzzer from the fuzzer list\n",
    "# and if you want to compare aflpp, honggfuzz, libfuzzer then exclude libafl-based fuzzer\n",
    "# It doesn't really make sense to mix them together, as we wrote in the paper \"rank is relative\".\n",
    "# They should be compared on the same baseline.\n",
    "# Yes, we presented the result of comparisons on afl++, honggfuzz, libfuzzer, though. but the main purpose of it is \n",
    "# more about confirming our study aligns with Dylan's work or work.\n",
    "\n",
    "# the list of fuzzers included in 22th May, 2023 experiment\n",
    "fuzzer_list_1 = [\n",
    "    # These three are disabled to see libafl-based fuzzer's rank correlation\n",
    "    # \"aflplusplus\",\n",
    "    # \"honggfuzz\",\n",
    "    # \"libfuzzer\",\n",
    "    \"libafl_fuzzbench_cov_accounting\",\n",
    "    \"libafl_fuzzbench_explore\",\n",
    "    \"libafl_fuzzbench_mopt\",\n",
    "    \"libafl_fuzzbench_value_profile\",\n",
    "    \"libafl_fuzzbench_weighted\",\n",
    "    \"libafl_fuzzbench_cmplog\",\n",
    "    \"libafl_fuzzbench_naive\",\n",
    "    \"libafl_fuzzbench_fast\",\n",
    "    \"libafl_fuzzbench_rand_scheduler\",\n",
    "]\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"./experiments/libafl0522.csv\", engine='python')\n",
    "selected = df[df['fuzzer'].isin(fuzzer_list_1)]\n",
    "selected = selected[['fuzzer', 'benchmark', 'edges_covered', 'time']]\n",
    "selected = selected[selected['time'] == TIME]\n",
    "\n",
    "benchmarks = selected.benchmark.unique()\n",
    "# Some experiment was not complete so we ran it again on 2th June, 2023\n",
    "fuzzer_list_2 = [\n",
    "    \"libafl_fuzzbench_ngram4\",\n",
    "    \"libafl_fuzzbench_ngram8\",\n",
    "    \"libafl_fuzzbench_naive_ctx\",\n",
    "]\n",
    "\n",
    "dff = pd.read_csv(\"./experiments/libafl0602.csv\", engine='python')\n",
    "selectedd = dff[dff['fuzzer'].isin(fuzzer_list_2)]\n",
    "selectedd = selectedd[['fuzzer', 'benchmark', 'edges_covered', 'time']]\n",
    "selectedd = selectedd[selectedd['time'] == TIME]\n",
    "\n",
    "# Lastly we added fix to grimoire fuzzer, so rerun the experiment on 25th September 2023 \n",
    "fuzzer_list_3 = [\n",
    "    \"libafl_fuzzbench_grimoire\",\n",
    "]\n",
    "dfff = pd.read_csv(\"./experiments/libafl0925.csv\", engine = 'python')\n",
    "selecteddd = dfff[dfff['fuzzer'].isin(fuzzer_list_3)]\n",
    "selecteddd = selecteddd[['fuzzer', 'benchmark', 'edges_covered', 'time']]\n",
    "selecteddd = selecteddd[selecteddd['time'] == TIME]\n",
    "\n",
    "\n",
    "result = pd.concat([selected, selectedd, selecteddd])\n",
    "fuzzer_list = fuzzer_list_1 + fuzzer_list_2 + fuzzer_list_3\n",
    "\n",
    "result.to_csv(\"result.csv\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ff0bc92-cbad-4a48-aee3-0f7b4d72406d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Spearman\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# mode = \"PERCENTILE\"\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "corr_result = dict()\n",
    "\n",
    "printf_debug = False\n",
    "# to color the graphs\n",
    "def get_cmap(n, name='hsv'):\n",
    "    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
    "    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n",
    "    return plt.cm.get_cmap(name, n)\n",
    "\n",
    "def run_analysis(file, property_data, filename):\n",
    "    if printf_debug:\n",
    "        print(\"Loading\", abs_path)\n",
    "    res_spearman = dict()\n",
    "    \n",
    "    for FUZZER in fuzzer_list:\n",
    "        points = dict()\n",
    "        for benchmark in benchmarks:\n",
    "            for fuzzer in fuzzer_list:\n",
    "                if fuzzer == FUZZER:\n",
    "                    # Select the result with this specific benchmark\n",
    "                    # The idea is to see\n",
    "                    # On benchmark A, what is the rank of the fuzzer X compared to all the other?\n",
    "                    data = result[(result['benchmark'] == benchmark)]\n",
    "                    property_rank, property_value = property_data[benchmark]\n",
    "\n",
    "                    # Rank them\n",
    "                    data.loc[:, 'fuzzer_rank'] = data.loc[:, 'edges_covered'].rank(method = 'average')\n",
    "                    # and get the target fuzzer\n",
    "                    data = data[(data['fuzzer'] == fuzzer)]\n",
    "\n",
    "                    for fuzzer_rank in data['fuzzer_rank']:\n",
    "                        if benchmark in points:\n",
    "                            points[benchmark].append((fuzzer_rank, property_rank))\n",
    "                        else:\n",
    "                            points[benchmark] = [(fuzzer_rank, property_rank)]\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        cmap = get_cmap(len(points.keys()))\n",
    "        for (i, (benchmark, vec)) in enumerate(points.items()):\n",
    "            for fuzzer_rank, property_rank in vec:\n",
    "                X.append(property_rank)\n",
    "                y.append(fuzzer_rank)\n",
    "            \n",
    "\n",
    "        reg = np.polyfit(X, y, 1)\n",
    "    \n",
    "        plt.scatter(X, y)\n",
    "        plt.xlabel(\"property rank\")\n",
    "        plt.ylabel(\"fuzzer rank\")\n",
    "        plt.title(\"{} {}\".format(filename, FUZZER))\n",
    "        # plt.show()\n",
    "        # plt.plot(X, f(X), color = \"r\")\n",
    "        from scipy import stats\n",
    "        import math\n",
    "\n",
    "        # Spearman\n",
    "        spe = stats.spearmanr(X, y)\n",
    "        # Kendall\n",
    "        tau = stats.kendalltau(X, y).statistic\n",
    "        spearman_r = spe.statistic\n",
    "        pvalue = spe.pvalue\n",
    "        count = len(X)\n",
    "        \n",
    "        res_spearman[FUZZER] = (spearman_r, tau, pvalue)\n",
    "        plt.annotate(text='r = {}, p = {}'.format(spearman_r, pvalue), xy=(0.03, 0.03), xycoords='figure fraction')\n",
    "        plt.clf()\n",
    "    \n",
    "    for (key, (r, tau, pvalue)) in res_spearman.items():\n",
    "        if key in corr_result:\n",
    "            corr_result[key].append((file, (r, tau, pvalue)))\n",
    "        else:\n",
    "            corr_result[key] = [(file, (r, tau, pvalue))]\n",
    "import os\n",
    "file_list = []\n",
    "# Place the data in '../data' to load\n",
    "for r, subdir, files in os.walk(\"../data\"):\n",
    "    for file in files:\n",
    "        ab = os.path.join(r, file)\n",
    "        file_list.append((ab, file))\n",
    "\n",
    "# we don't really need to sort but just for debug\n",
    "file_list.sort()\n",
    "for abs_path, file in file_list:\n",
    "    property_data = dict()\n",
    "    with open(abs_path) as f:\n",
    "        property_data = json.load(f)\n",
    "    assert(len(property_data) == 23)\n",
    "    run_analysis(ab, property_data, file)\n",
    "\n",
    "for k, v in corr_result.items():\n",
    "    v.sort(key = lambda x: x[1][0])\n",
    "    for nnn, (r, tau, _pvalue) in v:\n",
    "        if printf_debug:\n",
    "            print(k, nnn, r, tau)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8072abd8-ee3d-4c3b-a426-f3eb504b3ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
